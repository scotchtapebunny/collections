{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import genfromtxt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "#from acc_calc import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of inputs is X:  (24754, 784) \n",
      "size of labels is y:  (24754, 4)\n"
     ]
    }
   ],
   "source": [
    "#read all data in train_data.csv and train_labels.csv and save in x and y respectively\n",
    "X = genfromtxt('train_data.csv', delimiter=',', filling_values = 0)\n",
    "y = genfromtxt('train_labels.csv', delimiter=',', filling_values = 0)\n",
    "\n",
    "print(\"size of inputs is X: \", X.shape,\"\\nsize of labels is y: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train inputs is X_train:  (14852, 784) \n",
      "size of train labels is y:  (14852, 4) \n",
      "size of test inputs is X_test:  (9902, 784) \n",
      "size of train labels is y_test:  (9902, 4)\n"
     ]
    }
   ],
   "source": [
    "#using scikit learn to split data into training and learning\n",
    "\n",
    "# an 80/20 split was used (based on the pareto principle) initially, however not much difference was observed in accuracy with a 60/40 split, I chose to go for 60/40 in final run as it is less computationally intensive with a dataset of 24,000 records, additionally it gives us more data for validation. However, changing epochs from 100 to 150 did make a difference hence we chose to increase that instead\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "print(\"size of train inputs is X_train: \", X_train.shape,\"\\nsize of train labels is y: \", y_train.shape,\"\\nsize of test inputs is X_test: \", X_test.shape,\"\\nsize of train labels is y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "\n",
    "def sigmoid(A):\n",
    "    #suppress warnings needed because numpy uses float(64) by default and this function can return results larger than that\n",
    "    return 1 / (1 + np.exp(-A))\n",
    "\n",
    "#Derivative of sigmoid activation function\n",
    "def sigmoid_derivative(A):\n",
    "    return A * (1-A)\n",
    "\n",
    "# softmax activation function - used for output layer\n",
    "def softmax(A):\n",
    "    expA = np.exp(A - np.max (A))\n",
    "    return expA / expA.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#declaring the number of hidden layer neurons, classes, learning rate and number of epoch for the training\n",
    "#these values are selected after some trial and error and seem to give accuracy > 90\n",
    "n_hidden = 50\n",
    "num_classes = 4\n",
    "learning_rate = 0.0002\n",
    "epochs  = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer weights matrix dimension: (784, 50)\n",
      "Output layer weights matrix dimension: (50, 4)\n",
      "Hidden layer biases matrix dimension: (50,)\n",
      "Output layer biases matrix dimension: (4,)\n"
     ]
    }
   ],
   "source": [
    "#initialize weight matrix for hidden layer of size n_hidden x n_inputs (number of hidden layers) - we will use a numpy random matrix function that will generate a random matrix with values from a normal distribution\n",
    "\n",
    "n_inputs = X_train.shape[1] #number of features\n",
    "#print(n_inputs)\n",
    "\n",
    "#hidden layer weights of size (no of features x no of nodes) - this eliminates the need to transpose later\n",
    "weight_hidden = np.random.randn(n_inputs, n_hidden)\n",
    "#print(weight_hidden)\n",
    "print(\"Hidden layer weights matrix dimension: {}\".format(weight_hidden.shape))\n",
    "\n",
    "#initialize random weights for output layer of size num_classes x n_hidden\n",
    "weight_output = np.random.randn(n_hidden, num_classes)\n",
    "print(\"Output layer weights matrix dimension: {}\".format(weight_output.shape))\n",
    "#print(weight_output)\n",
    "\n",
    "#initilaize bias matrix of size 50x1 for hiddden layer\n",
    "hidden_bias = np.random.randn(n_hidden)\n",
    "print(\"Hidden layer biases matrix dimension: {}\".format(hidden_bias.shape))\n",
    "\n",
    "#initilaize bias matrix of size 4x1 for output layer\n",
    "output_bias = np.random.randn(num_classes)\n",
    "print(\"Output layer biases matrix dimension: {}\".format(output_bias.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "\n",
    "def forward(X_training_data, hidden_layer_weights, output_layer_weights, hidden_layer_bias, output_layer_bias):\n",
    "    hidden_layer_Sum = np.dot(X_training_data, hidden_layer_weights) + hidden_layer_bias\n",
    "    hidden_layer_Sig = sigmoid(hidden_layer_Sum)\n",
    "    output_layer_Sum = np.dot(hidden_layer_Sig, output_layer_weights) + output_layer_bias\n",
    "    final_output = softmax(output_layer_Sum)\n",
    "\n",
    "    #we are interested in getting the final output and output of hidden layer from forward propagation to use it for backpropagation\n",
    "    return final_output, hidden_layer_Sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "def backward(X_training_data, y_training_data, hidden_layer_output, output_weight_matrix, act_output):\n",
    "    #calculating weight delta for output layer\n",
    "    output_error = act_output - y_training_data\n",
    "    delta_weights_output = np.dot(hidden_layer_output.T, output_error)\n",
    "\n",
    "    #calculating weight delta for hidden layer\n",
    "    hidden_layer_error = np.dot(output_error, output_weight_matrix.T)\n",
    "    hidden_layer_error_output_mul = sigmoid(hidden_layer_output) * hidden_layer_error #just an intermediate step\n",
    "    delta_weights_hidden = np.dot(X_training_data.T, hidden_layer_error_output_mul)\n",
    "\n",
    "    #calculating delta biases for output and hidden layer\n",
    "    delta_bias_output = output_error\n",
    "    delta_bias_hidden = hidden_layer_error_output_mul\n",
    "\n",
    "    return delta_weights_output, delta_weights_hidden, delta_bias_output, delta_bias_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch No.: 0 , Accuracy: 24.46%\n",
      "Epoch No.: 10 , Accuracy: 85.26%\n",
      "Epoch No.: 20 , Accuracy: 91.03%\n",
      "Epoch No.: 30 , Accuracy: 92.77%\n",
      "Epoch No.: 40 , Accuracy: 93.21%\n",
      "Epoch No.: 50 , Accuracy: 93.44%\n",
      "Epoch No.: 60 , Accuracy: 93.89%\n",
      "Epoch No.: 70 , Accuracy: 94.58%\n",
      "Epoch No.: 80 , Accuracy: 94.68%\n",
      "Epoch No.: 90 , Accuracy: 94.86%\n",
      "Model Training Completed\n"
     ]
    }
   ],
   "source": [
    "#main program\n",
    "\n",
    "#suppressing warnings for float overflow when using exp in sigmoid function\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range (epochs):\n",
    "    #forward run through network\n",
    "    network_output, hidden_layer_output = forward(X_train, weight_hidden, weight_output, hidden_bias, output_bias)\n",
    "\n",
    "    #backward run through network\n",
    "    delta_output_weights, delta_hidden_weights, delta_output_bias, delta_hidden_bias = backward(X_train, y_train, hidden_layer_output, weight_output, network_output)\n",
    "\n",
    "    #update weights as per the delta derived from back propagation\n",
    "    weight_hidden = weight_hidden - learning_rate * delta_hidden_weights\n",
    "    hidden_bias = hidden_bias - learning_rate * delta_hidden_bias.sum(axis = 0)\n",
    "    weight_output = weight_output - learning_rate * delta_output_weights\n",
    "    output_bias = output_bias - learning_rate * delta_output_bias.sum(axis = 0)\n",
    "\n",
    "\n",
    "    #The code below will run test data through FFN after every 10 epochs to see improving accuracy - this is commented out to make epochs quicker - uncomment to see accuracy as epochs run\n",
    "\n",
    "    if i % 10 == 0:\n",
    "\n",
    "        y_predicted= forward(X_test, weight_hidden, weight_output, hidden_bias, output_bias)[0] #extract first return value of forward function\n",
    "\n",
    "        #onehot encode the output\n",
    "        #reference - https://www.adamsmith.haus/python/answers/how-to-replace-elements-in-a-numpy-array-if-a-condition-is-met-in-python\n",
    "        y_predicted = np.where(y_predicted < 0.5, 0, y_predicted)\n",
    "        y_predicted = np.where(y_predicted >= 0.5, 1, y_predicted)\n",
    "        print(\"Epoch No.: {} , Accuracy: {:.2f}%\".format(i, accuracy_score(y_test, y_predicted) * 100))\n",
    "        #print(\"Epoch No.: {} , Accuracy: {:.2f}%\".format(i, accuracy(y_test, y_predicted) * 100))\n",
    "\n",
    "print(\"Model Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Prediction Accuracy: 95.02%\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "(9902, 4)\n"
     ]
    }
   ],
   "source": [
    "#Run test input through trained model and see accuracy\n",
    "\n",
    "y_predicted= forward(X_test, weight_hidden, weight_output, hidden_bias, output_bias)[0] #extract first return value of forward function\n",
    "\n",
    "#onehot encode the output\n",
    "#reference - https://www.adamsmith.haus/python/answers/how-to-replace-elements-in-a-numpy-array-if-a-condition-is-met-in-python\n",
    "y_predicted = np.where(y_predicted < 0.5, 0, y_predicted)\n",
    "y_predicted = np.where(y_predicted >= 0.5, 1, y_predicted)\n",
    "\n",
    "print(\"Test Data Prediction Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_predicted) * 100))\n",
    "#print(\"Epoch No.: {} , Accuracy: {:.2f}%\".format(i, accuracy(y_test, y_predicted) * 100))\n",
    "print(y_predicted)\n",
    "print(y_predicted.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.save('Trained_weight_hidden.npy', weight_hidden)\n",
    "np.save('Trained_bias_hidden.npy', hidden_bias)\n",
    "np.save('Trained_weight_output.npy', weight_output)\n",
    "np.save('Trained_bias_output.npy', output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecda20238df2ed8e9d74a8333869d6ab36a80806eec0e5f6f1311a96726d7d59"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "ecda20238df2ed8e9d74a8333869d6ab36a80806eec0e5f6f1311a96726d7d59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}